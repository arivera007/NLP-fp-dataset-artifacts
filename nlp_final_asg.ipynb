{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "mount_file_id": "1aIsDT-xC6p2CUxMTroIPQNdQA7V293Ax",
      "authorship_tag": "ABX9TyOayUHfRNRR4zw67CY8zCJl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arivera007/NLP-fp-dataset-artifacts/blob/main/nlp_final_asg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3xzedoRoeMAI"
      },
      "outputs": [],
      "source": [
        "/content/drive/MyDrive/nlp_final"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/arivera007/NLP-fp-dataset-artifacts.git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HR9V1nBUseQU",
        "outputId": "f032ddc6-14b5-46b6-b38c-ed603c27cd18"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'NLP-fp-dataset-artifacts'...\n",
            "remote: Enumerating objects: 41, done.\u001b[K\n",
            "remote: Counting objects: 100% (41/41), done.\u001b[K\n",
            "remote: Compressing objects: 100% (21/21), done.\u001b[K\n",
            "remote: Total 41 (delta 14), reused 41 (delta 14), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (41/41), 15.39 KiB | 15.39 MiB/s, done.\n",
            "Resolving deltas: 100% (14/14), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YsVnSzhbaP1F",
        "outputId": "c36d9294-57c6-4c3d-db53-caa7093b270b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (24.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-24.3.1-py3-none-any.whl.metadata (3.7 kB)\n",
            "Downloading pip-24.3.1-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m69.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 24.1.2\n",
            "    Uninstalling pip-24.1.2:\n",
            "      Successfully uninstalled pip-24.1.2\n",
            "Successfully installed pip-24.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd NLP-fp-dataset-artifacts\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-0xw8Y7bIxx",
        "outputId": "8dceafd0-4d79-4c16-d369-1ab454096944"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/NLP-fp-dataset-artifacts\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (1.1.1)\n",
            "Collecting datasets (from -r requirements.txt (line 2))\n",
            "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (2.5.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (4.66.6)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (4.46.2)\n",
            "Collecting evaluate (from -r requirements.txt (line 6))\n",
            "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate->-r requirements.txt (line 1)) (0.26.2)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate->-r requirements.txt (line 1)) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate->-r requirements.txt (line 1)) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate->-r requirements.txt (line 1)) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate->-r requirements.txt (line 1)) (6.0.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate->-r requirements.txt (line 1)) (0.4.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 2)) (3.16.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 2)) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets->-r requirements.txt (line 2))\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 2)) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 2)) (2.32.3)\n",
            "Collecting xxhash (from datasets->-r requirements.txt (line 2))\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets->-r requirements.txt (line 2))\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets->-r requirements.txt (line 2))\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 2)) (3.11.2)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 3)) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 3)) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 3)) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 3)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->-r requirements.txt (line 3)) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 5)) (2024.9.11)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 5)) (0.20.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets->-r requirements.txt (line 2)) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets->-r requirements.txt (line 2)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets->-r requirements.txt (line 2)) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets->-r requirements.txt (line 2)) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->-r requirements.txt (line 3)) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->-r requirements.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->-r requirements.txt (line 2)) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->-r requirements.txt (line 2)) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->-r requirements.txt (line 2)) (1.16.0)\n",
            "Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
            "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
            "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "Downloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "Installing collected packages: xxhash, fsspec, dill, multiprocess, datasets, evaluate\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.1.0 dill-0.3.8 evaluate-0.4.3 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8UtB5y2bOdO",
        "outputId": "bce4485a-c108-46db-935a-9a84f6216aaa"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "helpers.py  README.md  requirements.txt  run.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 run.py --do_train --task nli --dataset snli --output_dir ./trained_model3/ --num_train_epochs 1.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OihnHmFVbqBN",
        "outputId": "25371b6a-d146-4aeb-e716-f1b932dd09d1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-11-22 21:48:46.496365: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-11-22 21:48:46.515563: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-11-22 21:48:46.521411: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-11-22 21:48:46.535653: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-11-22 21:48:47.527097: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Preprocessing data... (this takes a little bit, should only happen once per dataset)\n",
            "/content/NLP-fp-dataset-artifacts/run.py:159: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = trainer_class(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mletsgotravel007-github\u001b[0m (\u001b[33mletsgotravel007-github-university-of-texas-at-austin\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/NLP-fp-dataset-artifacts/wandb/run-20241122_214900-xg73eevx\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m./trained_model3/\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/letsgotravel007-github-university-of-texas-at-austin/huggingface\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/letsgotravel007-github-university-of-texas-at-austin/huggingface/runs/xg73eevx\u001b[0m\n",
            "{'loss': 0.9247, 'grad_norm': 41.69772720336914, 'learning_rate': 4.963594530442254e-05, 'epoch': 0.01}\n",
            "{'loss': 0.7237, 'grad_norm': 22.488534927368164, 'learning_rate': 4.927189060884507e-05, 'epoch': 0.01}\n",
            "{'loss': 0.6972, 'grad_norm': 12.832856178283691, 'learning_rate': 4.890783591326761e-05, 'epoch': 0.02}\n",
            "{'loss': 0.6424, 'grad_norm': 17.50347328186035, 'learning_rate': 4.8543781217690146e-05, 'epoch': 0.03}\n",
            "{'loss': 0.6462, 'grad_norm': 40.41703796386719, 'learning_rate': 4.817972652211268e-05, 'epoch': 0.04}\n",
            "{'loss': 0.6228, 'grad_norm': 12.834125518798828, 'learning_rate': 4.781567182653522e-05, 'epoch': 0.04}\n",
            "{'loss': 0.5974, 'grad_norm': 13.882363319396973, 'learning_rate': 4.7451617130957756e-05, 'epoch': 0.05}\n",
            "{'loss': 0.6077, 'grad_norm': 7.688723564147949, 'learning_rate': 4.70875624353803e-05, 'epoch': 0.06}\n",
            "{'loss': 0.5856, 'grad_norm': 19.721879959106445, 'learning_rate': 4.672350773980283e-05, 'epoch': 0.07}\n",
            "{'loss': 0.5793, 'grad_norm': 17.006675720214844, 'learning_rate': 4.6359453044225366e-05, 'epoch': 0.07}\n",
            "{'loss': 0.5778, 'grad_norm': 6.93428373336792, 'learning_rate': 4.599539834864791e-05, 'epoch': 0.08}\n",
            "{'loss': 0.5713, 'grad_norm': 5.223712921142578, 'learning_rate': 4.563134365307044e-05, 'epoch': 0.09}\n",
            "{'loss': 0.5668, 'grad_norm': 19.912111282348633, 'learning_rate': 4.5267288957492976e-05, 'epoch': 0.09}\n",
            "{'loss': 0.5489, 'grad_norm': 9.386674880981445, 'learning_rate': 4.490323426191551e-05, 'epoch': 0.1}\n",
            "{'loss': 0.57, 'grad_norm': 2.4833695888519287, 'learning_rate': 4.4539179566338044e-05, 'epoch': 0.11}\n",
            "{'loss': 0.5515, 'grad_norm': 38.0346794128418, 'learning_rate': 4.4175124870760585e-05, 'epoch': 0.12}\n",
            "{'loss': 0.5261, 'grad_norm': 6.842548847198486, 'learning_rate': 4.381107017518312e-05, 'epoch': 0.12}\n",
            "{'loss': 0.5413, 'grad_norm': 11.870367050170898, 'learning_rate': 4.3447015479605654e-05, 'epoch': 0.13}\n",
            "{'loss': 0.5433, 'grad_norm': 11.885286331176758, 'learning_rate': 4.3082960784028195e-05, 'epoch': 0.14}\n",
            "{'loss': 0.526, 'grad_norm': 27.85967254638672, 'learning_rate': 4.2718906088450736e-05, 'epoch': 0.15}\n",
            "{'loss': 0.5234, 'grad_norm': 18.053632736206055, 'learning_rate': 4.235485139287327e-05, 'epoch': 0.15}\n",
            "{'loss': 0.5272, 'grad_norm': 5.61905574798584, 'learning_rate': 4.1990796697295805e-05, 'epoch': 0.16}\n",
            "{'loss': 0.5456, 'grad_norm': 14.161932945251465, 'learning_rate': 4.162674200171834e-05, 'epoch': 0.17}\n",
            "{'loss': 0.5048, 'grad_norm': 9.856029510498047, 'learning_rate': 4.126268730614088e-05, 'epoch': 0.17}\n",
            "{'loss': 0.5198, 'grad_norm': 4.0076727867126465, 'learning_rate': 4.0898632610563415e-05, 'epoch': 0.18}\n",
            "{'loss': 0.5208, 'grad_norm': 10.917736053466797, 'learning_rate': 4.053457791498595e-05, 'epoch': 0.19}\n",
            "{'loss': 0.5139, 'grad_norm': 12.765534400939941, 'learning_rate': 4.017052321940848e-05, 'epoch': 0.2}\n",
            "{'loss': 0.4968, 'grad_norm': 4.886528015136719, 'learning_rate': 3.980646852383102e-05, 'epoch': 0.2}\n",
            "{'loss': 0.5214, 'grad_norm': 13.603167533874512, 'learning_rate': 3.944241382825356e-05, 'epoch': 0.21}\n",
            "{'loss': 0.5009, 'grad_norm': 24.429628372192383, 'learning_rate': 3.907835913267609e-05, 'epoch': 0.22}\n",
            "{'loss': 0.4924, 'grad_norm': 34.09567642211914, 'learning_rate': 3.8714304437098634e-05, 'epoch': 0.23}\n",
            "{'loss': 0.4952, 'grad_norm': 9.300237655639648, 'learning_rate': 3.835024974152117e-05, 'epoch': 0.23}\n",
            "{'loss': 0.4951, 'grad_norm': 7.501761436462402, 'learning_rate': 3.798619504594371e-05, 'epoch': 0.24}\n",
            "{'loss': 0.4863, 'grad_norm': 14.853608131408691, 'learning_rate': 3.7622140350366244e-05, 'epoch': 0.25}\n",
            "{'loss': 0.4933, 'grad_norm': 11.327095985412598, 'learning_rate': 3.725808565478878e-05, 'epoch': 0.25}\n",
            "{'loss': 0.4752, 'grad_norm': 16.26304054260254, 'learning_rate': 3.689403095921131e-05, 'epoch': 0.26}\n",
            "{'loss': 0.5006, 'grad_norm': 20.973726272583008, 'learning_rate': 3.6529976263633853e-05, 'epoch': 0.27}\n",
            "{'loss': 0.4829, 'grad_norm': 12.642570495605469, 'learning_rate': 3.616592156805639e-05, 'epoch': 0.28}\n",
            "{'loss': 0.4803, 'grad_norm': 6.5410308837890625, 'learning_rate': 3.580186687247892e-05, 'epoch': 0.28}\n",
            "{'loss': 0.5019, 'grad_norm': 2.8541815280914307, 'learning_rate': 3.5437812176901456e-05, 'epoch': 0.29}\n",
            "{'loss': 0.4912, 'grad_norm': 17.096176147460938, 'learning_rate': 3.507375748132399e-05, 'epoch': 0.3}\n",
            "{'loss': 0.4949, 'grad_norm': 6.969968795776367, 'learning_rate': 3.470970278574653e-05, 'epoch': 0.31}\n",
            "{'loss': 0.4829, 'grad_norm': 2.7549920082092285, 'learning_rate': 3.4345648090169066e-05, 'epoch': 0.31}\n",
            "{'loss': 0.4748, 'grad_norm': 5.956436634063721, 'learning_rate': 3.398159339459161e-05, 'epoch': 0.32}\n",
            "{'loss': 0.4739, 'grad_norm': 12.934272766113281, 'learning_rate': 3.361753869901414e-05, 'epoch': 0.33}\n",
            "{'loss': 0.4657, 'grad_norm': 13.911951065063477, 'learning_rate': 3.325348400343668e-05, 'epoch': 0.33}\n",
            "{'loss': 0.4603, 'grad_norm': 4.179773807525635, 'learning_rate': 3.288942930785922e-05, 'epoch': 0.34}\n",
            "{'loss': 0.473, 'grad_norm': 17.46795654296875, 'learning_rate': 3.252537461228175e-05, 'epoch': 0.35}\n",
            "{'loss': 0.4974, 'grad_norm': 4.374216079711914, 'learning_rate': 3.2161319916704286e-05, 'epoch': 0.36}\n",
            "{'loss': 0.4575, 'grad_norm': 8.077094078063965, 'learning_rate': 3.179726522112683e-05, 'epoch': 0.36}\n",
            "{'loss': 0.4384, 'grad_norm': 5.45765495300293, 'learning_rate': 3.143321052554936e-05, 'epoch': 0.37}\n",
            "{'loss': 0.4633, 'grad_norm': 30.440338134765625, 'learning_rate': 3.1069155829971895e-05, 'epoch': 0.38}\n",
            "{'loss': 0.4963, 'grad_norm': 11.699840545654297, 'learning_rate': 3.070510113439443e-05, 'epoch': 0.39}\n",
            "{'loss': 0.4653, 'grad_norm': 24.060123443603516, 'learning_rate': 3.0341046438816967e-05, 'epoch': 0.39}\n",
            "{'loss': 0.4586, 'grad_norm': 35.710845947265625, 'learning_rate': 2.997699174323951e-05, 'epoch': 0.4}\n",
            "{'loss': 0.4648, 'grad_norm': 71.13704681396484, 'learning_rate': 2.9612937047662043e-05, 'epoch': 0.41}\n",
            "{'loss': 0.4781, 'grad_norm': 11.571011543273926, 'learning_rate': 2.9248882352084577e-05, 'epoch': 0.42}\n",
            "{'loss': 0.4729, 'grad_norm': 14.593384742736816, 'learning_rate': 2.888482765650711e-05, 'epoch': 0.42}\n",
            "{'loss': 0.4583, 'grad_norm': 12.618110656738281, 'learning_rate': 2.8520772960929652e-05, 'epoch': 0.43}\n",
            "{'loss': 0.461, 'grad_norm': 22.698183059692383, 'learning_rate': 2.815671826535219e-05, 'epoch': 0.44}\n",
            "{'loss': 0.4685, 'grad_norm': 9.031845092773438, 'learning_rate': 2.7792663569774724e-05, 'epoch': 0.44}\n",
            "{'loss': 0.4353, 'grad_norm': 26.983339309692383, 'learning_rate': 2.742860887419726e-05, 'epoch': 0.45}\n",
            "{'loss': 0.4615, 'grad_norm': 2.835705041885376, 'learning_rate': 2.70645541786198e-05, 'epoch': 0.46}\n",
            "{'loss': 0.4732, 'grad_norm': 12.788654327392578, 'learning_rate': 2.6700499483042334e-05, 'epoch': 0.47}\n",
            "{'loss': 0.4592, 'grad_norm': 13.383965492248535, 'learning_rate': 2.633644478746487e-05, 'epoch': 0.47}\n",
            "{'loss': 0.4448, 'grad_norm': 22.485139846801758, 'learning_rate': 2.5972390091887406e-05, 'epoch': 0.48}\n",
            "{'loss': 0.4296, 'grad_norm': 54.07405090332031, 'learning_rate': 2.560833539630994e-05, 'epoch': 0.49}\n",
            "{'loss': 0.4485, 'grad_norm': 6.63787317276001, 'learning_rate': 2.524428070073248e-05, 'epoch': 0.5}\n",
            "{'loss': 0.4434, 'grad_norm': 1.0439356565475464, 'learning_rate': 2.4880226005155016e-05, 'epoch': 0.5}\n",
            "{'loss': 0.4459, 'grad_norm': 24.30409812927246, 'learning_rate': 2.451617130957755e-05, 'epoch': 0.51}\n",
            "{'loss': 0.4627, 'grad_norm': 2.9767580032348633, 'learning_rate': 2.4152116614000088e-05, 'epoch': 0.52}\n",
            "{'loss': 0.4431, 'grad_norm': 4.514057159423828, 'learning_rate': 2.3788061918422626e-05, 'epoch': 0.52}\n",
            "{'loss': 0.475, 'grad_norm': 4.079630374908447, 'learning_rate': 2.3424007222845163e-05, 'epoch': 0.53}\n",
            "{'loss': 0.4584, 'grad_norm': 8.360882759094238, 'learning_rate': 2.3059952527267698e-05, 'epoch': 0.54}\n",
            "{'loss': 0.4341, 'grad_norm': 9.029921531677246, 'learning_rate': 2.2695897831690235e-05, 'epoch': 0.55}\n",
            "{'loss': 0.4455, 'grad_norm': 9.307913780212402, 'learning_rate': 2.233184313611277e-05, 'epoch': 0.55}\n",
            "{'loss': 0.4537, 'grad_norm': 16.003517150878906, 'learning_rate': 2.1967788440535304e-05, 'epoch': 0.56}\n",
            "{'loss': 0.4616, 'grad_norm': 7.904518127441406, 'learning_rate': 2.1603733744957845e-05, 'epoch': 0.57}\n",
            "{'loss': 0.4341, 'grad_norm': 17.660573959350586, 'learning_rate': 2.123967904938038e-05, 'epoch': 0.58}\n",
            "{'loss': 0.4458, 'grad_norm': 10.06944465637207, 'learning_rate': 2.0875624353802917e-05, 'epoch': 0.58}\n",
            "{'loss': 0.4189, 'grad_norm': 32.160133361816406, 'learning_rate': 2.051156965822545e-05, 'epoch': 0.59}\n",
            "{'loss': 0.4409, 'grad_norm': 1.5026036500930786, 'learning_rate': 2.014751496264799e-05, 'epoch': 0.6}\n",
            "{'loss': 0.4341, 'grad_norm': 19.71647834777832, 'learning_rate': 1.9783460267070523e-05, 'epoch': 0.6}\n",
            "{'loss': 0.4511, 'grad_norm': 2.721637010574341, 'learning_rate': 1.941940557149306e-05, 'epoch': 0.61}\n",
            "{'loss': 0.4325, 'grad_norm': 15.755187034606934, 'learning_rate': 1.90553508759156e-05, 'epoch': 0.62}\n",
            "{'loss': 0.4489, 'grad_norm': 5.067020893096924, 'learning_rate': 1.8691296180338137e-05, 'epoch': 0.63}\n",
            "{'loss': 0.4298, 'grad_norm': 5.36005163192749, 'learning_rate': 1.832724148476067e-05, 'epoch': 0.63}\n",
            "{'loss': 0.4269, 'grad_norm': 9.844069480895996, 'learning_rate': 1.796318678918321e-05, 'epoch': 0.64}\n",
            "{'loss': 0.455, 'grad_norm': 8.988951683044434, 'learning_rate': 1.7599132093605743e-05, 'epoch': 0.65}\n",
            "{'loss': 0.4212, 'grad_norm': 5.785454750061035, 'learning_rate': 1.723507739802828e-05, 'epoch': 0.66}\n",
            "{'loss': 0.4382, 'grad_norm': 17.543811798095703, 'learning_rate': 1.687102270245082e-05, 'epoch': 0.66}\n",
            "{'loss': 0.4342, 'grad_norm': 11.112530708312988, 'learning_rate': 1.6506968006873353e-05, 'epoch': 0.67}\n",
            "{'loss': 0.4396, 'grad_norm': 11.26253604888916, 'learning_rate': 1.614291331129589e-05, 'epoch': 0.68}\n",
            "{'loss': 0.439, 'grad_norm': 3.788414239883423, 'learning_rate': 1.5778858615718425e-05, 'epoch': 0.68}\n",
            "{'loss': 0.42, 'grad_norm': 5.646763324737549, 'learning_rate': 1.5414803920140962e-05, 'epoch': 0.69}\n",
            "{'loss': 0.4343, 'grad_norm': 0.6795647144317627, 'learning_rate': 1.5050749224563498e-05, 'epoch': 0.7}\n",
            "{'loss': 0.4202, 'grad_norm': 26.333316802978516, 'learning_rate': 1.4686694528986036e-05, 'epoch': 0.71}\n",
            "{'loss': 0.4231, 'grad_norm': 9.760356903076172, 'learning_rate': 1.4322639833408572e-05, 'epoch': 0.71}\n",
            "{'loss': 0.4405, 'grad_norm': 2.514301300048828, 'learning_rate': 1.395858513783111e-05, 'epoch': 0.72}\n",
            "{'loss': 0.4358, 'grad_norm': 17.381677627563477, 'learning_rate': 1.3594530442253644e-05, 'epoch': 0.73}\n",
            "{'loss': 0.4344, 'grad_norm': 13.934096336364746, 'learning_rate': 1.3230475746676182e-05, 'epoch': 0.74}\n",
            "{'loss': 0.4387, 'grad_norm': 16.205976486206055, 'learning_rate': 1.2866421051098718e-05, 'epoch': 0.74}\n",
            "{'loss': 0.4013, 'grad_norm': 17.456632614135742, 'learning_rate': 1.2502366355521252e-05, 'epoch': 0.75}\n",
            "{'loss': 0.4389, 'grad_norm': 6.039938449859619, 'learning_rate': 1.213831165994379e-05, 'epoch': 0.76}\n",
            "{'loss': 0.4175, 'grad_norm': 26.098587036132812, 'learning_rate': 1.1774256964366328e-05, 'epoch': 0.76}\n",
            "{'loss': 0.4155, 'grad_norm': 21.86076545715332, 'learning_rate': 1.1410202268788864e-05, 'epoch': 0.77}\n",
            "{'loss': 0.413, 'grad_norm': 10.472549438476562, 'learning_rate': 1.10461475732114e-05, 'epoch': 0.78}\n",
            "{'loss': 0.4294, 'grad_norm': 23.157880783081055, 'learning_rate': 1.0682092877633937e-05, 'epoch': 0.79}\n",
            "{'loss': 0.3972, 'grad_norm': 8.831790924072266, 'learning_rate': 1.0318038182056472e-05, 'epoch': 0.79}\n",
            "{'loss': 0.4189, 'grad_norm': 1.6650525331497192, 'learning_rate': 9.95398348647901e-06, 'epoch': 0.8}\n",
            "{'loss': 0.413, 'grad_norm': 9.86148452758789, 'learning_rate': 9.589928790901545e-06, 'epoch': 0.81}\n",
            "{'loss': 0.4092, 'grad_norm': 12.779573440551758, 'learning_rate': 9.225874095324081e-06, 'epoch': 0.82}\n",
            "{'loss': 0.427, 'grad_norm': 5.856677532196045, 'learning_rate': 8.861819399746617e-06, 'epoch': 0.82}\n",
            "{'loss': 0.4219, 'grad_norm': 18.00519371032715, 'learning_rate': 8.497764704169155e-06, 'epoch': 0.83}\n",
            "{'loss': 0.4035, 'grad_norm': 6.170464515686035, 'learning_rate': 8.133710008591691e-06, 'epoch': 0.84}\n",
            "{'loss': 0.4082, 'grad_norm': 6.457894325256348, 'learning_rate': 7.769655313014227e-06, 'epoch': 0.84}\n",
            "{'loss': 0.4271, 'grad_norm': 14.518421173095703, 'learning_rate': 7.405600617436764e-06, 'epoch': 0.85}\n",
            "{'loss': 0.4349, 'grad_norm': 13.835975646972656, 'learning_rate': 7.041545921859301e-06, 'epoch': 0.86}\n",
            "{'loss': 0.4123, 'grad_norm': 9.153889656066895, 'learning_rate': 6.677491226281838e-06, 'epoch': 0.87}\n",
            "{'loss': 0.4039, 'grad_norm': 16.36705207824707, 'learning_rate': 6.313436530704374e-06, 'epoch': 0.87}\n",
            "{'loss': 0.4071, 'grad_norm': 9.95065689086914, 'learning_rate': 5.94938183512691e-06, 'epoch': 0.88}\n",
            "{'loss': 0.4024, 'grad_norm': 1.5941684246063232, 'learning_rate': 5.5853271395494466e-06, 'epoch': 0.89}\n",
            "{'loss': 0.4203, 'grad_norm': 1.0701676607131958, 'learning_rate': 5.2212724439719826e-06, 'epoch': 0.9}\n",
            "{'loss': 0.4059, 'grad_norm': 6.359682083129883, 'learning_rate': 4.857217748394519e-06, 'epoch': 0.9}\n",
            "{'loss': 0.4271, 'grad_norm': 11.394402503967285, 'learning_rate': 4.4931630528170554e-06, 'epoch': 0.91}\n",
            "{'loss': 0.3981, 'grad_norm': 6.014419078826904, 'learning_rate': 4.1291083572395914e-06, 'epoch': 0.92}\n",
            "{'loss': 0.4196, 'grad_norm': 14.45718765258789, 'learning_rate': 3.7650536616621283e-06, 'epoch': 0.92}\n",
            "{'loss': 0.4031, 'grad_norm': 2.07069993019104, 'learning_rate': 3.4009989660846647e-06, 'epoch': 0.93}\n",
            "{'loss': 0.3995, 'grad_norm': 7.281665325164795, 'learning_rate': 3.036944270507201e-06, 'epoch': 0.94}\n",
            "{'loss': 0.4059, 'grad_norm': 5.401233673095703, 'learning_rate': 2.6728895749297376e-06, 'epoch': 0.95}\n",
            "{'loss': 0.4061, 'grad_norm': 20.874141693115234, 'learning_rate': 2.308834879352274e-06, 'epoch': 0.95}\n",
            "{'loss': 0.4334, 'grad_norm': 17.312049865722656, 'learning_rate': 1.9447801837748105e-06, 'epoch': 0.96}\n",
            "{'loss': 0.4207, 'grad_norm': 15.132023811340332, 'learning_rate': 1.5807254881973467e-06, 'epoch': 0.97}\n",
            "{'loss': 0.3856, 'grad_norm': 11.562432289123535, 'learning_rate': 1.2166707926198831e-06, 'epoch': 0.98}\n",
            "{'loss': 0.4011, 'grad_norm': 8.846638679504395, 'learning_rate': 8.526160970424198e-07, 'epoch': 0.98}\n",
            "{'loss': 0.3838, 'grad_norm': 33.397830963134766, 'learning_rate': 4.885614014649561e-07, 'epoch': 0.99}\n",
            "{'loss': 0.4176, 'grad_norm': 20.129013061523438, 'learning_rate': 1.2450670588749254e-07, 'epoch': 1.0}\n",
            "{'train_runtime': 2783.6158, 'train_samples_per_second': 197.357, 'train_steps_per_second': 24.67, 'train_loss': 0.47185929974129076, 'epoch': 1.0}\n",
            "100% 68671/68671 [46:22<00:00, 24.68it/s]\n",
            "\u001b[1;34mwandb\u001b[0m: üöÄ View run \u001b[33m./trained_model3/\u001b[0m at: \u001b[34mhttps://wandb.ai/letsgotravel007-github-university-of-texas-at-austin/huggingface/runs/xg73eevx\u001b[0m\n",
            "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20241122_214900-xg73eevx/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 run.py --do_eval --task nli --dataset snli --model ./trained_model3/ --output_dir ./eval_output3/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QXxr-7Gvbvb3",
        "outputId": "40f572ff-e548-459f-c000-db431a9a46b3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-11-22 22:36:59.844575: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-11-22 22:36:59.864013: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-11-22 22:36:59.870044: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-11-22 22:36:59.884998: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-11-22 22:37:00.864742: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Preprocessing data... (this takes a little bit, should only happen once per dataset)\n",
            "/content/NLP-fp-dataset-artifacts/run.py:159: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = trainer_class(\n",
            " 99% 1224/1231 [00:15<00:00, 80.47it/s]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mletsgotravel007-github\u001b[0m (\u001b[33mletsgotravel007-github-university-of-texas-at-austin\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/NLP-fp-dataset-artifacts/wandb/run-20241122_223728-bfuuaylg\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m./eval_output3/\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/letsgotravel007-github-university-of-texas-at-austin/huggingface\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/letsgotravel007-github-university-of-texas-at-austin/huggingface/runs/bfuuaylg\u001b[0m\n",
            "100% 1231/1231 [00:17<00:00, 72.26it/s]\n",
            "Evaluation results:\n",
            "{'eval_loss': 0.35017141699790955, 'eval_model_preparation_time': 0.003, 'eval_accuracy': 0.8811216950416565, 'eval_runtime': 16.1962, 'eval_samples_per_second': 607.674, 'eval_steps_per_second': 76.006}\n",
            "\u001b[1;34mwandb\u001b[0m: üöÄ View run \u001b[33m./eval_output3/\u001b[0m at: \u001b[34mhttps://wandb.ai/letsgotravel007-github-university-of-texas-at-austin/huggingface/runs/bfuuaylg\u001b[0m\n",
            "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20241122_223728-bfuuaylg/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_TJZoTB140t",
        "outputId": "37ec4794-f9e5-4742-cabe-6de135b365ec"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eval_output3  helpers.py  __pycache__  README.md  requirements.txt  run.py  trained_model3  wandb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r /content/NLP-fp-dataset-artifacts/eval_output3 /content/drive/MyDrive/nlp_final"
      ],
      "metadata": {
        "id": "ubuKffEL16Ek"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r /content/NLP-fp-dataset-artifacts/trained_model3 /content/drive/MyDrive/nlp_final"
      ],
      "metadata": {
        "id": "79nwpnkCKbfT"
      },
      "execution_count": 12,
      "outputs": []
    }
  ]
}